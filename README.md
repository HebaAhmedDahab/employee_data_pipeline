# Employee Data Pipeline ğŸš€

A production-ready data engineering pipeline that extracts employee data from SQL Server, transforms it through bronze/silver/gold layers, and creates analytics-ready datasets.

## ğŸ“‹ Project Overview

This project demonstrates a modern data engineering workflow using the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) to process employee data from the AdventureWorksDW2022 database.

### Architecture

```
SQL Server (Source)
    â†“
BRONZE LAYER (Raw Data)
    â†“
SILVER LAYER (Cleaned & Validated)
    â†“
GOLD LAYER (Analytics-Ready)
```

### What This Pipeline Does

1. **Extract**: Pulls employee and department data from SQL Server
2. **Transform**: Cleans, standardizes, and enriches the data
3. **Load**: Creates business-ready analytics tables
4. **Monitor**: Logs all operations and validates data quality

---

## ğŸ› ï¸ Tech Stack

- **Language**: Python 3.8+
- **Database**: SQL Server (AdventureWorksDW2022)
- **Libraries**:
  - `pandas` - Data manipulation
  - `pyodbc` - SQL Server connectivity
  - `SQLAlchemy` - Database abstraction
  - `python-dotenv` - Environment management
  - `great-expectations` - Data validation (optional)

---

## ğŸ“ Project Structure

```
employee-data-pipeline/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ db_config.py          # Database connection configuration
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract/              # Data extraction modules
â”‚   â”‚   â”œâ”€â”€ extract_employees.py
â”‚   â”‚   â””â”€â”€ extract_departments.py
â”‚   â”‚
â”‚   â”œâ”€â”€ transform/            # Data transformation modules
â”‚   â”‚   â””â”€â”€ transform_employees.py
â”‚   â”‚
â”‚   â”œâ”€â”€ load/                 # Data loading modules
â”‚   â”‚   â””â”€â”€ load_to_gold.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                # Utility modules
â”‚       â”œâ”€â”€ logger.py         # Logging configuration
â”‚       â””â”€â”€ data_quality.py   # Data quality checks
â”‚
â”œâ”€â”€ data/                     # Data storage (not in git)
â”‚   â”œâ”€â”€ bronze/               # Raw extracted data
â”‚   â”œâ”€â”€ silver/               # Cleaned data
â”‚   â””â”€â”€ gold/                 # Analytics-ready data
â”‚
â”œâ”€â”€ logs/                     # Pipeline logs (not in git)
â”œâ”€â”€ tests/                    # Unit tests
â”œâ”€â”€ notebooks/                # Jupyter notebooks for exploration
â”‚
â”œâ”€â”€ main_pipeline.py          # Main pipeline orchestrator
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .env.example              # Environment variables template
â”œâ”€â”€ .gitignore               # Git ignore rules
â””â”€â”€ README.md                # This file
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8 or higher
- SQL Server with AdventureWorksDW2022 database
- SQL Server ODBC Driver 17

### Installation

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd employee-data-pipeline
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   
   # On Windows
   venv\Scripts\activate
   
3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables**
   ```bash
   # Copy the example file
   copy .env.example .env  # Windows
      
   # Edit .env with your database settings
   # For Windows Authentication, leave USERNAME and PASSWORD empty
   ```

### Configuration

Edit your `.env` file:

---

## ğŸƒ Running the Pipeline

### Test Database Connection

```bash
python config/db_config.py
```

Expected output:
```
Testing SQL Server connection...
Server: BEBA
Database: AdventureWorksDW2022
âœ… Connection successful!
```

### Run Individual Components

**Extract Only:**
```bash
python src/extract/extract_employees.py
python src/extract/extract_departments.py
```

**Transform Only:**
```bash
python src/transform/transform_employees.py
```

**Load Only:**
```bash
python src/load/load_to_gold.py
```

### Run Complete Pipeline

```bash
python main_pipeline.py
```

Expected output:
```
============================================================
ğŸš€ STARTING PIPELINE: Employee Data Pipeline
Start Time: 2024-01-30 10:30:15
============================================================

PHASE 1: EXTRACTION
============================================================
âœ… Employees extracted: 290 rows
âœ… Departments extracted: 5 rows

PHASE 2: TRANSFORMATION
============================================================
âœ… Employees transformed: 290 rows

PHASE 3: LOAD TO GOLD LAYER
============================================================
âœ… Analytics tables created: 4

ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY! ğŸ‰
```

---

## ğŸ“Š Output Data

### Bronze Layer (`data/bronze/`)
- **Raw data** directly from SQL Server
- No transformations applied
- Includes extraction timestamp

**Files:**
- `dimemployee_latest.csv` - Employee data
- `dimdepartmentgroup_latest.csv` - Department data

### Silver Layer (`data/silver/`)
- **Cleaned and standardized** data
- Duplicates removed
- Data types corrected
- Derived fields added (Age, YearsOfService, FullName)

**Files:**
- `employees_latest.csv` - Cleaned employee data

### Gold Layer (`data/gold/`)
- **Analytics-ready** aggregated tables
- Business metrics and KPIs

**Files:**
- `department_summary_latest.csv` - Department-level metrics
- `gender_diversity_latest.csv` - Gender distribution by department
- `tenure_analysis_latest.csv` - Employee tenure breakdown
- `hiring_trends_latest.csv` - Hiring patterns by year

---

## ğŸ§ª Testing the Pipeline

### Test with New Data

1. Open SQL Server Management Studio
2. Add a new employee record:
   ```sql
   -- Example: Add a test employee
   INSERT INTO DimEmployee (...)
   VALUES (...)
   ```
3. Run the pipeline again:
   ```bash
   python main_pipeline.py
   ```
4. Check if the new record appears in the output

### Validate Data Quality

The pipeline automatically performs quality checks:
- âœ… Row count validation
- âœ… Null value detection
- âœ… Duplicate detection
- âœ… Data type validation

Check logs in `logs/pipeline_YYYYMMDD.log`

---

## ğŸ“ Logging

All pipeline operations are logged to:
- **Console** - Real-time output
- **Log files** - `logs/pipeline_YYYYMMDD.log`

Log levels:
- `INFO` - Normal operations
- `WARNING` - Data quality issues
- `ERROR` - Pipeline failures

---

## ğŸ” Data Quality Checks

The pipeline includes comprehensive quality checks:

```python
# Example quality check output
[DimEmployee] Row count: 290 âœ…
[DimEmployee] âš ï¸ Column 'EmailAddress' has 5 null values (1.72%)
[DimEmployee] âœ… No duplicates found
```

---

## ğŸ“ˆ Analytics Examples

### Department Summary
```
DepartmentName  | total_employees | avg_base_rate | avg_years_of_service
----------------|-----------------|---------------|---------------------
Engineering     | 75              | 32.50         | 8.5
Production      | 120             | 28.75         | 6.2
Marketing       | 45              | 35.00         | 5.8
```

### Gender Diversity
```
DepartmentName  | Gender | employee_count | percentage
----------------|--------|----------------|------------
Engineering     | Male   | 60             | 80.00
Engineering     | Female | 15             | 20.00
```

---

## ğŸ”„ Next Steps & Improvements

### Phase 2 Enhancements
- [ ] Add Apache Airflow for scheduling
- [ ] Implement incremental loads
- [ ] Add email notifications
- [ ] Create Power BI dashboards

### Phase 3 Enhancements
- [ ] Deploy to cloud (AWS/Azure/GCP)
- [ ] Add CI/CD pipeline
- [ ] Implement data versioning
- [ ] Add real-time streaming

---

## ğŸ¤ Contributing

This is a learning project. Feel free to:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

## ğŸ› Troubleshooting

### Connection Issues

**Error**: `pyodbc.InterfaceError: ('IM002'...)`
- **Solution**: Install ODBC Driver 17 for SQL Server

**Error**: `Login failed for user`
- **Solution**: Verify Windows Authentication is enabled

### Import Errors

**Error**: `ModuleNotFoundError: No module named 'pandas'`
- **Solution**: Install requirements: `pip install -r requirements.txt`

### Data Issues

**Error**: `FileNotFoundError: Bronze file not found`
- **Solution**: Run extraction first: `python src/extract/extract_employees.py`

---

## ğŸ¯ Project Goals Achieved

âœ… Extract data from SQL Server using Python
âœ… Implement Medallion Architecture (Bronze/Silver/Gold)
âœ… Perform data quality checks
âœ… Create analytics-ready datasets
âœ… Implement comprehensive logging
âœ… Follow software engineering best practices
âœ… Make GitHub-ready with proper documentation

---
